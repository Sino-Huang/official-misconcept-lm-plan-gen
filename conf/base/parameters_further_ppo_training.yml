# This is a boilerplate parameters config generated for pipeline 'further_ppo_training'
# using Kedro 0.19.6.
#
# Documentation for this file format can be found in "Parameters"
# Link: https://docs.kedro.org/en/0.19.6/configuration/parameters.html

further_ppo_training_params:
  # ! we further train the model with PPO for those with t4 (scratchpad) data augmentation 

  t0_params:
    checkpoint_dir: "data/07_model_output/qwen/full_t0/sft"
    config_type_name: "t0"
    is_llama_factory: true
    response_length: 2600
    lora_rank: 128
    lora_alpha: 256
    per_device_train_batch_size: 1
    if_qlora: false 
    gradient_accumulation_steps: 4

  accu_t1_t4_params:
    checkpoint_dir: "data/07_model_output/qwen/full_accu_t1_t4/sft"
    config_type_name: "accu_t1_t4"
    is_llama_factory: true
    response_length: 2600
    lora_rank: 128
    lora_alpha: 256
    per_device_train_batch_size: 1
    if_qlora: false 
    gradient_accumulation_steps: 4


  t0_t5_params:
    checkpoint_dir: "data/07_model_output/qwen/full_t0_t5/sft"
    config_type_name: "t0_t5"
    is_llama_factory: true
    response_length: 3000
    lora_rank: 128
    lora_alpha: 256
    per_device_train_batch_size: 1
    if_qlora: false 
    gradient_accumulation_steps: 4

  # ! BELOW GET OOM 

  accu_t2_t4_params:
    checkpoint_dir: "data/07_model_output/qwen/full_accu_t2_t4/sft"
    config_type_name: "accu_t2_t4"
    is_llama_factory: true
    response_length: 11860 # 11860
    lora_rank: 16 # 32 qlora , 16 lora 
    lora_alpha: 32 # 64 qlora , 32 lora
    per_device_train_batch_size: 1
    if_qlora: false 
    gradient_accumulation_steps: 2

  accu_t1_t3_t4_params:
    checkpoint_dir: ""
    config_type_name: "accu_t1_t3_t4"
    is_llama_factory: true
    response_length: 10703
    lora_rank: 128
    lora_alpha: 256
    per_device_train_batch_size: 1
    if_qlora: false 
    gradient_accumulation_steps: 2

  accu_t4_params:
    checkpoint_dir: ""
    config_type_name: "accu_t4"
    is_llama_factory: true
    response_length: 23340
    lora_rank: 128
    lora_alpha: 256
    per_device_train_batch_size: 1
    if_qlora: false 
    gradient_accumulation_steps: 2



    
further_ppo_training_general_params:
  wandb_record: true 
  huggingface_dataset_dir: "${training_llm_for_plan_generation_params.huggingface_dataset_dir}"
  ppo_data_cache_dir: "data/02_intermediate/ppo_data_cache"

  planning_domain_config_dir: "${training_llm_for_plan_generation_params.planning_domain_config_dir}"
  domain_fp_pattern: "${training_llm_for_plan_generation_params.domain_fp_pattern}"
  problem_instance_fp_pattern: "${training_llm_for_plan_generation_params.problem_instance_fp_pattern}"

  plan_output_dir: "${training_llm_for_plan_generation_params.plan_output_dir}"
  val_fp: "${training_llm_for_plan_generation_params.val_fp}"

  seq_params: "${training_llm_for_plan_generation_params.seq_params}"
  dtype: "${training_llm_for_plan_generation_params.dtype}"

  seed: 23
  example_per_domain: 20  # 10% of the total examples 
  num_ppo_epochs: 1
  num_mini_batches: 1
  learning_rate: 3.0e-6
  total_episodes: 500000
  model_name_or_path: "Qwen/Qwen2-7B-Instruct"
  missing_eos_penalty: 0.5
  stop_token: "eos"
  ddp_timeout: 180000000
  save_steps: 250 # default: 250
  save_total_limit: 1
  

  lora_dropout: 0.05
  
  mode: "rloo" # "ppo" or "rloo"

  resume_checkpoint_path: ""
  




# Debugging t0 test_longer_horizon
# Recommended response_length for qwen: 764
# Debugging accu_t1 test_longer_horizon
# Recommended response_length for qwen: 764
# Debugging accu_t2 test_longer_horizon
# Recommended response_length for qwen: 9474
# Debugging accu_t3 test_longer_horizon
# Recommended response_length for qwen: 18046
# Debugging accu_t4 test_longer_horizon
# Recommended response_length for qwen: 23340
# Debugging accu_t1_t4 test_longer_horizon
# Recommended response_length for qwen: 949
# Debugging accu_t2_t4 test_longer_horizon
# Recommended response_length for qwen: 11860
# Debugging accu_t1_t3 test_longer_horizon
# Recommended response_length for qwen: 9256
# Debugging accu_t1_t3_t4 test_longer_horizon
# Recommended response_length for qwen: 10703