## Data generator 

we will use [Rao's LLM Planning Benchmark](https://github.com/karthikv792/LLMs-Planning/tree/main/plan-bench) dataset to generate training corpus for our language model.

## Before you start
please check the code in `*.py` files to make sure what you are doing.

> [!IMPORTANT]  
> please download and install FAST_DOWNWARD and VAL and assign the path to environmental variables
> please ensure that we have run `pip install "tarski==0.7.0"` to install the tarski library, you may want to add `--force` to force the installation.

`task_1` is what we are interested in.

- `llm_plan_pipeline.py`: console script to run the pipeline
- `obfuscator.py`: obfuscate the plan, not relevant for now
- `problem_generators.py`: to automate the generation of problem instances for planning domains, ensuring that the instances are unique and valid according to specified criteria. They by default only support **blocksworld** and **logistics** domains. To support more domains, please check [PDDL generator](https://github.com/AI-Planning/pddl-generators)
- `prompt_generation.py`: responsible for generating prompts for various tasks related to planning in the context of PDDL, also responsible for generating plan and plan text.
- `response_generation.py`: ask LLMs to generate responses for the prompts generated by `prompt_generation.py`, we do not need this.
- `response_evaluation.py`: have `text_to_plan` function to convert text to plan, and then eval the plan, we will use it to evaluate the generated plan.

## How 
go to the repository `./data/01_raw/LLMs-Planning/plan-bench`

### Prompt generation 
```bash
python prompt_generation.py --task t1 --config blocksworld --verbose True
```

### problem generation 
**Blockworld**
```bash
python problem_generators.py --config blocksworld --n_instances 4200 --max_blocks 12 --min_length 3 --max_length 16

python problem_generators.py --config blocksworld --n_instances 200 --max_blocks 12 --min_length 17 --max_length 32

python problem_generators.py --config mystery_blocksworld --n_instances 200 --max_blocks 12 --min_length 3 --max_length 16
```

**Logistics**
```bash
python problem_generators.py --config logistics --n_instances 4200 --min_length 3 --max_length 16 --MAX_REPEAT_COUNT 800

python problem_generators.py --config logistics --n_instances 200 --min_length 17 --max_length 32 --MAX_REPEAT_COUNT 10

python problem_generators.py --config obfuscated_deceptive_logistics --n_instances 200 --min_length 3 --max_length 16 --MAX_REPEAT_COUNT 400
```

**Hanoi (Unseen Domain)**
```bash
python problem_generators.py --config hanoi --n_instances 200 --min_length 3 --max_length 16
```

**Storage (Unseen Domain)**
```bash
python problem_generators.py --config storage --n_instances 200 --min_length 3 --max_length 16 --MAX_REPEAT_COUNT 200
```

**Depots**
```bash
python problem_generators.py --config depots --n_instances 4200 --max_blocks 12 --min_length 3 --max_length 16 --TIME_CONSUMING_LIMIT 2 --TIMEOUT 7

python problem_generators.py --config depots --n_instances 200 --max_blocks 12 --min_length 17 --max_length 32 --TIME_CONSUMING_LIMIT 4 --TIMEOUT 20 --test
```

**Barman**
```bash
python problem_generators.py --config barman --n_instances 4200 --min_length 3 --max_length 17 --TIME_CONSUMING_LIMIT 2 --TIMEOUT 7 --REFRESH_LIMIT 20 

python problem_generators.py --config barman --n_instances 200 --min_length 17 --max_length 32 --TIME_CONSUMING_LIMIT 4 --TIMEOUT 20 --REFRESH_LIMIT 20 --test
```

**Childsnack**
```bash
python problem_generators.py --config childsnack --n_instances 4200 --min_length 3 --max_length 17 --TIME_CONSUMING_LIMIT 2 --TIMEOUT 7 --REFRESH_LIMIT 50

python problem_generators.py --config childsnack --n_instances 200 --min_length 17 --max_length 32 --TIME_CONSUMING_LIMIT 2 --TIMEOUT 20 --REFRESH_LIMIT 20 --test
```

**Grippers**
```bash
python problem_generators.py --config grippers --n_instances 4200 --min_length 3 --max_length 16 --TIME_CONSUMING_LIMIT 2 --TIMEOUT 7 --REFRESH_LIMIT 40

python problem_generators.py --config grippers --n_instances 200 --min_length 17 --max_length 32 --TIME_CONSUMING_LIMIT 2 --TIMEOUT 20 --REFRESH_LIMIT 20 --test
```

**Driverlog**
```bash
python problem_generators.py --config driverlog --n_instances 4200 --min_length 3 --max_length 16 --TIME_CONSUMING_LIMIT 2 --TIMEOUT 7 --REFRESH_LIMIT 40 --CHUNK_SIZE 3

python problem_generators.py --config driverlog --n_instances 200 --min_length 17 --max_length 33 --TIME_CONSUMING_LIMIT 2 --TIMEOUT 20 --REFRESH_LIMIT 20 --CHUNK_SIZE 3 --test
```

**Satellite**
```bash
python problem_generators.py --config satellite --n_instances 4200 --min_length 3 --max_length 16 --TIME_CONSUMING_LIMIT 2 --TIMEOUT 7 --REFRESH_LIMIT 40 

python problem_generators.py --config satellite --n_instances 200 --min_length 17 --max_length 32 --TIME_CONSUMING_LIMIT 2 --TIMEOUT 20 --REFRESH_LIMIT 20 --test
```

> [!IMPORTANT]
> Once you complete the generation of the problem instances under the `data/01_raw/LLMs-Planning/plan-bench`, you shall continue to generate natural language descriptions for the generated problem instances.


## Prompt Template

> [!IMPORTANT]
> Text Prompt Generation will be done in the Kedro pipeline, where the instance generation were done in the `data/01_raw/LLMs-Planning/plan-bench` folder.

```python
structured_output = {
    "domain": domain_name,
    "domain_type": 'normal' # 'normal' | 'mystery' | 'novel'
    "type_id": ['t0', 'accu-t1', 'accu-t2', 'accu-t3', 'accu-t4', 'accu-t4', 'accu-t1+t4', 'accu-t2+t4'] # t0 is the raw prompt, t1 is the prompt with shuffled information (data augmentation), t2 is the lite CoT in [PLAN] section, t3 is the dense CoT information in [PLAN] section, t4 contains the "scratchpad" states. 
    "instance_id": instance_id,
    "full_prompt_lst": [prompt_1, ..., prompt_n],
    "prompt_num": n, # t0 num is 1, t1 num is 10, t2 num is also 10, t3 num is also 10, t4 num is 10 * 6 = 60 but for each prompt, we have 50% chances for each sentence to have additional mistakes-correction retry sentence. 
    "domain_text": query_text, # part of the prompt
    "raw_plan": [action_1, ..., action_n],
    "plan_text": plan_text, # part of the prompt
    "plan_num": m # number of actions in the plan
}
```

## Details of different type of prompts 
## Stage 0 raw text sequence

## Stage 1 training data with some reasonable data augmentation on the [DOMAIN] section

Why data augmentation:

- Data augmentation helps neural networks learn features that are invariant to certain transformations.

Reasonable Augmentation in the domain descriptions

- permute the order of mentioned actions.
- permute the order of the conditions


Purpose of this data augmentation:

- **Learning semantic invariance**: The order of mentioning actions, restrictions, or conditions doesn't change the underlying meaning or logic of the problem.

Justification for using this type of data augmentation 

- **Natural language usage pattern**: In real-world scenarios, people often express the same information in different orders. This augmentation mimics natural language variation, making the model more robust to diverse phrasings.

## Stage 2 lite CoT in [PLAN] section

- write down the following two components {(1) goal state, (2) how many steps left} before we provide the action
- this encourages LM to concern about the state transitions and estimate the progress
- **Building towards CoT (Implicit)**
    - **Step Count to detect whether goal is reached:**
        - If the estimated number of steps left increases or doesn't decrease as expected, it's a clear signal that something might be wrong with the plan.
        - Example: If the LM estimates 5 steps left, then takes an action and still estimates 5 steps left, it suggests the action might not have been productive.
    - **Implicit shortest CoT by repeating goal state:**
        - Having goal state will emphasize the goal target. Thus by repeating them, we are forcing the model to associate the actual plan sequence with the goal. 
        

## Stage 3 adding dense CoT in [PLAN] section

- before giving the action name in the step, write down the precondition of this action (e.g., I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block.)
- after giving the action name in the step, write down the effect of this action (e.g., Once I pick up or unstack a block, I am holding the block.)

## Stage DEPRECATED shuffle the elements in each plan step

- again, we are learning the semantic invariance

## Stage 4 mistake-correction process

Consider an example with offline planning trajectory training data:

```jsx
A -> B -> C -> D
```

To create training data for mistake correction, we intentionally introduce errors by **moving later action steps forward** in the sequence.

```jsx
C [back] -> B [back] -> A -> B -> C -> D
```

- If Stage 2 is activated (which involves predicting how many steps are left), For each step in the mistake-augmented sequence, we assign a "steps left" (SL) value, which can either be:
    - the step left of the sentence in the original true sequence, denoted as `SL-true`
    - the local step left number, which can be expressed as `Len - index of current sentence` , where `Len`  is the length of the original true sequence

## Example:

Let's go through the example with the provided sequences:

1. **Original Sequence:**
    - `A (SL-true: 3) -> B (SL-true: 2) -> C (SL-true: 1) -> D (SL-true: 0)`
    - `Len = 4`
2. **Mistake Sequence:**
    - `C [back] -> B [back] -> A -> B -> C -> D`
3. **Step Prediction for Mistake Sequence:**
    - `C (SL-aug: 50% chance of 3 (Len - 1) or 1 (SL-true))`
    - `B (SL-aug: 50% chance of 2 (Len - 2) or 2 (SL-true))`
    - `A (SL-aug: 3)`
    - `B (SL-aug: 2)`
    - `C (SL-aug: 1)`
    - `D (SL-aug: 0)`


## Error-Correction Synthesized Data Generation Details 

There are two type of mistakes we can introduce in the synthesized data:
1. state transition is illegal / not desired 
   - synthesize the state transition that appears later in the plan to appear earlier in the plan
     - the model has to generalize the state transition because negative example templates can be extremely large and we cannot cover all the possible negative examples
2. cannot detect if the goal is reached
   - create negative examples with the remaining step counter to be incorrect


# Huggingface Dataset Creation
## split the dataset into sets
ref: https://huggingface.co/docs/datasets/load_hub 

we shall split `Splits` it into 
```python
[
    "train"
    "test:same domain",
    "test:unseen domain",
    "test:longer horizon",
    "test:mystery domain"
]
```

## set diff configs
we shall split `Configurations` into
```python
[
    "t0: raw prompt (t0)"
    "accu_t1: t0 + shuffled domain information (t1)"
    "accu_t2: t0 + t1 + lite CoT in [PLAN] section (t2)"
    "accu_t3: t0 + t1 + t2 + dense CoT information in [PLAN] section (t3)"
    "accu_t4: t0 + t1 + t2 + t3 + error-correction scratchpad (t4)"
    "accu_t1_t4: t0 + t1 + t4"
    "accu_t2_t4: t0 + t1 + t2 + t4"
    "accu_t1_t3: t0 + t1 + t3"
    "accu_t1_t3_t4: t0 + t1 + t3 + t4"
]
```

## features
Features are actually the `columns` in the DataFrame. We shall have the following features 
```python
[
    "domain",
    "instance_id", # map to the problem instance, for train dataset we range from 0 to 3999, for test dataset we range from 0 to 199
    "prompt_id", # can range from 0 to 25, as we are making variances for the problem instance
    "domain_text",
    "plan_text",
    "plan_length",
    "raw_plan",
    "full_text_prompt" # this is domain_text + plan_text
]
```

## Fast Tokenization
Model cannot directly process the text, we need to convert the text into tokens. We shall use the `tokenizer` to convert the text into tokens. 

```python
def tokenization(example):

    return tokenizer(example["text"])

dataset = dataset.map(tokenization, batched=True)
```
## Create a dataset loading script 
ref: https://huggingface.co/docs/datasets/main/en/repository_structure
optional ref: https://huggingface.co/docs/datasets/main/en/dataset_script

